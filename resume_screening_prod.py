
"""
Resume Screening - Production Ready Script
Author: Generated by ChatGPT (assist)
Description:
    - Loads resume dataset (CSV)
    - Cleans and preprocesses text
    - Trains a classification pipeline (TF-IDF + classifier)
    - Evaluates and saves model artifacts (pipeline, label encoder)
    - Provides CLI for training and single-prediction inference

Usage examples:
    Train:
        python resume_screening_prod.py --mode train --data-path /path/to/sample.csv --target-column Category --text-column Update_Resume --output-dir ./artifacts

    Predict:
        python resume_screening_prod.py --mode predict --input-text "Experienced Python developer ..." --model-dir ./artifacts

Notes:
    - Expects a CSV file with at least text and label columns.
    - Saves artifacts as joblib files in the specified output directory.
"""

from __future__ import annotations
import argparse
import logging
import os
from pathlib import Path
import re
import sys
from typing import List, Tuple, Optional

import joblib
import numpy as np
import pandas as pd

# Text processing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# NLP
import spacy

# ML
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# ----------------------------
# Configuration & Logging
# ----------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)],
)

logger = logging.getLogger("resume_screening")

# ----------------------------
# Utility: Ensure NLTK/Spacy models
# ----------------------------
def ensure_nltk_and_spacy():
    try:
        _ = stopwords.words("english")
    except LookupError:
        logger.info("Downloading NLTK stopwords and punkt...")
        nltk.download("stopwords")
        nltk.download("punkt")

    try:
        spacy.load("en_core_web_sm")
    except OSError:
        logger.info("Downloading spaCy 'en_core_web_sm' model...")
        from spacy.cli import download as spacy_download
        spacy_download("en_core_web_sm")

ensure_nltk_and_spacy()
NLP = spacy.load("en_core_web_sm", disable=["parser", "ner"])

# ----------------------------
# TextCleaner Transformer
# ----------------------------
class TextCleaner(BaseEstimator, TransformerMixin):
    """
    Custom transformer to clean and preprocess resume text.
    - Removes non-alphabetic characters
    - Lowercases
    - Removes URLs and extra whitespace
    - Tokenizes and removes stopwords
    - Returns joined cleaned text
    """
    def __init__(self, remove_stopwords: bool = True):
        self.remove_stopwords = remove_stopwords
        self.stopwords = set(stopwords.words("english")) if remove_stopwords else set()

    def _clean_text(self, text: str) -> str:
        if pd.isna(text):
            return ""
        # Remove URLs
        text = re.sub(r"http\S+", " ", str(text))
        # Replace non-alpha with spaces and lowercase
        text = re.sub(r"[^a-zA-Z]", " ", text)
        text = re.sub(r"\s+", " ", text).strip().lower()
        tokens = word_tokenize(text)
        if self.remove_stopwords:
            tokens = [t for t in tokens if t not in self.stopwords and len(t) > 1]
        # Optional: use spaCy for sentence segmentation or lemmatization (kept simple for speed)
        return " ".join(tokens)

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [self._clean_text(x) for x in X]

# ----------------------------
# Core functions
# ----------------------------
def load_data(csv_path: str, text_column: str, target_column: str) -> pd.DataFrame:
    df = pd.read_csv(csv_path)
    if text_column not in df.columns or target_column not in df.columns:
        raise ValueError(f"Required columns not found in data. Available columns: {df.columns.tolist()}")
    df = df[[text_column, target_column]].dropna(subset=[text_column])
    df = df.rename(columns={text_column: "text", target_column: "label"})
    logger.info("Loaded dataset with %d rows", len(df))
    return df

def build_pipeline(classifier: str = "logreg") -> Pipeline:
    """
    Build a sklearn Pipeline: TextCleaner -> TfidfVectorizer -> Classifier
    classifier: 'logreg' (default), 'rf'
    """
    if classifier == "logreg":
        clf = LogisticRegression(max_iter=1000, random_state=42)
    elif classifier == "rf":
        clf = RandomForestClassifier(n_estimators=200, random_state=42)
    else:
        raise ValueError("Supported classifiers: 'logreg', 'rf'")

    pipeline = Pipeline([
        ("cleaner", TextCleaner()),
        ("tfidf", TfidfVectorizer(max_df=0.9, min_df=2, ngram_range=(1,2))),
        ("clf", clf),
    ])
    return pipeline

def train_and_evaluate(df: pd.DataFrame, output_dir: str, classifier: str = "logreg", test_size: float = 0.2, random_state: int = 42) -> dict:
    X = df["text"].astype(str)
    y = df["label"].astype(str)

    le = LabelEncoder()
    y_enc = le.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=test_size, random_state=random_state, stratify=y_enc)

    pipeline = build_pipeline(classifier=classifier)
    logger.info("Training pipeline with classifier=%s ...", classifier)
    pipeline.fit(X_train, y_train)

    logger.info("Evaluating on test set...")
    preds = pipeline.predict(X_test)
    acc = accuracy_score(y_test, preds)
    report = classification_report(y_test, preds, target_names=le.classes_, zero_division=0)
    logger.info("Test Accuracy: %.4f", acc)
    logger.info("Classification Report:\n%s", report)

    # Save artifacts
    out_path = Path(output_dir)
    out_path.mkdir(parents=True, exist_ok=True)
    pipeline_file = out_path / "pipeline.joblib"
    labelenc_file = out_path / "label_encoder.joblib"

    joblib.dump(pipeline, pipeline_file)
    joblib.dump(le, labelenc_file)

    logger.info("Saved pipeline to %s and label encoder to %s", pipeline_file, labelenc_file)

    return {
        "accuracy": acc,
        "report": report,
        "pipeline_path": str(pipeline_file),
        "label_encoder_path": str(labelenc_file),
    }

def predict_single(text: str, model_dir: str) -> Tuple[str, np.ndarray]:
    pipeline_file = Path(model_dir) / "pipeline.joblib"
    labelenc_file = Path(model_dir) / "label_encoder.joblib"
    if not pipeline_file.exists() or not labelenc_file.exists():
        raise FileNotFoundError("Model artifacts not found in model_dir: " + str(model_dir))
    pipeline = joblib.load(pipeline_file)
    le = joblib.load(labelenc_file)
    cleaned = pipeline.named_steps["cleaner"].transform([text])
    pred_enc = pipeline.predict(cleaned)
    pred_label = le.inverse_transform(pred_enc)[0]
    # Also return class probabilities if supported
    probs = None
    if hasattr(pipeline, "predict_proba"):
        probs = pipeline.predict_proba(cleaned)[0]
    return pred_label, probs

# ----------------------------
# CLI
# ----------------------------
def parse_args():
    parser = argparse.ArgumentParser(description="Resume Screening - Train and Predict")
    parser.add_argument("--mode", choices=["train", "predict"], required=True, help="Mode: train or predict")
    parser.add_argument("--data-path", type=str, help="Path to CSV data for training")
    parser.add_argument("--text-column", type=str, default="Update_Resume", help="Name of text column in CSV")
    parser.add_argument("--target-column", type=str, default="Category", help="Name of target/label column in CSV")
    parser.add_argument("--output-dir", type=str, default="./artifacts", help="Directory to save trained model artifacts")
    parser.add_argument("--classifier", choices=["logreg", "rf"], default="logreg", help="Classifier to use")
    parser.add_argument("--input-text", type=str, help="Text for single prediction (predict mode)")
    return parser.parse_args()

def main():
    args = parse_args()
    if args.mode == "train":
        if not args.data_path:
            raise ValueError("Please provide --data-path for training mode")
        df = load_data(args.data_path, text_column=args.text_column, target_column=args.target_column)
        results = train_and_evaluate(df, output_dir=args.output_dir, classifier=args.classifier)
        logger.info("Training completed. Results: %s", results)
    elif args.mode == "predict":
        if not args.input_text:
            raise ValueError("Please provide --input-text for predict mode")
        label, probs = predict_single(args.input_text, model_dir=args.output_dir)
        print("Predicted label:", label)
        if probs is not None:
            print("Class probabilities:", probs)

if __name__ == "__main__":
    main()
